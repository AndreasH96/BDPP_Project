{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialize Spark and the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark Lab1, master=local[*]) created by __init__ at <ipython-input-1-482db03b4247>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-482db03b4247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Spark Lab1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    333\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 335\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    336\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark Lab1, master=local[*]) created by __init__ at <ipython-input-1-482db03b4247>:7 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('Spark Lab1')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Lab1\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data and plot initial data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset Length: 9002021\n",
      "Active Dataset Length: 100000\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "|         US|   16414143|R3W4P9UBGNGH1U|B00YL0EKWE|     852431543|LG G4 Case Hard T...|        Wireless|          2|            1|          3|   N|                Y|Looks good, funct...|2 issues  -  Once...| 2015-08-31|\n",
      "|         US|   50800750|R15V54KBMTQWAY|B00XK95RPQ|     516894650|Selfie Stick Fibl...|        Wireless|          4|            0|          0|   N|                N| A fun little gadget|I’m embarrassed t...| 2015-08-31|\n",
      "|         US|   15184378| RY8I449HNXSVF|B00SXRXUKO|     984297154|Tribe AB40 Water ...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|  Fits iPhone 6 well| 2015-08-31|\n",
      "|         US|   10203548|R18TLJYCKJFLSR|B009V5X1CE|     279912704|RAVPower® Element...|        Wireless|          5|            0|          0|   N|                Y|       Great charger|Great charger.  I...| 2015-08-31|\n",
      "|         US|     488280|R1NK26SWS53B8Q|B00D93OVF0|     662791300|Fosmon Micro USB ...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|Great for the pri...| 2015-08-31|\n",
      "|         US|   13334021|R11LOHEDYJALTN|B00XVGJMDQ|     421688488|iPhone 6 Case, Vo...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|Great Case, bette...| 2015-08-31|\n",
      "|         US|   27520697|R3ALQVQB2P9LA7|B00KQW1X1C|     554285554|Nokia Lumia 630 R...|        Wireless|          4|            0|          0|   N|                Y|          Four Stars|Easy to set up an...| 2015-08-31|\n",
      "|         US|   48086021|R3MWLXLNO21PDQ|B00IP1MQNK|     488006702|Lumsing 10400mah ...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|         Works great| 2015-08-31|\n",
      "|         US|   12738196|R2L15IS24CX0LI|B00HVORET8|     389677711|iPhone 5S Battery...|        Wireless|          5|            0|          0|   N|                Y|      So far so good|So far so good. I...| 2015-08-31|\n",
      "|         US|   15867807|R1DJ8976WPWVZU|B00HX3G6J6|     299654876|HTC One M8 Screen...|        Wireless|          3|            0|          0|   N|                Y|seems durable but...|seems durable but...| 2015-08-31|\n",
      "|         US|    1972249|R3MRWNNR8CBTB7|B00U4NATNQ|     577878727|S6 Case - Bear Mo...|        Wireless|          5|            0|          0|   N|                Y|Super thin, light...|Super thin, light...| 2015-08-31|\n",
      "|         US|   10956619|R1DS6DKTUXAQK3|B00SZEFDH8|     654620704|BLU Studio X, Unl...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|As good as the Sa...| 2015-08-31|\n",
      "|         US|   14805911| RWJM5E0TWUJD2|B00JRJUL9U|     391166958|EZOPower 5-Port U...|        Wireless|          5|            0|          0|   N|                Y|            EZOPower|I received this o...| 2015-08-31|\n",
      "|         US|   15611116|R1XTJKDYNCRGAC|B00KQ4T0HE|     481551630|iPhone 6S Case &i...|        Wireless|          1|            0|          0|   N|                Y|Very cheap case. ...|Very cheap case. ...| 2015-08-31|\n",
      "|         US|   39298603|R2UZL3DPWEU1XW|B00M0YWKPM|     685107474| iPhone 6s Plus Case|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|       son loves it.| 2015-08-31|\n",
      "|         US|   17552454|R2EZXET9KBFFU3|B00KDZEE68|     148320945|zBoost ZB575-A TR...|        Wireless|          1|            0|          0|   N|                Y|            One Star|you have to turn ...| 2015-08-31|\n",
      "|         US|   12218556|R26VY1L1FD3LPU|B00BJN45GM|      47788188|OtterBox Defender...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|  Awesome, thank you| 2015-08-31|\n",
      "|         US|   21872923|R2SSA4NSFCV18T|B00SA86SXW|     748759272|Aduro PowerUP 30W...|        Wireless|          5|            0|          0|   N|                N|          Five Stars|         nice, smart| 2015-08-31|\n",
      "|         US|   16264332|R1G6333JHJNEUQ|B00Q3I68TU|     974085141|LilGadgets Connec...|        Wireless|          5|            0|          0|   N|                Y|Great headphones ...|We love these hea...| 2015-08-31|\n",
      "|         US|    6042304|R2DRG0UZXJQ0PE|B00TN4J1TA|     716174627|Anker Aluminum Mu...|        Wireless|          5|            0|          0|   N|                Y|          Five Stars|This is well wort...| 2015-08-31|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "dataDF = spark.read.options(sep='\\t',header=True).csv('amazon_reviews_us_Wireless_v1_00.tsv')\n",
    "print(\"Total Dataset Length: {}\".format(dataDF.count()))\n",
    "dataDF.createOrReplaceTempView(\"reviewData\")\n",
    "reviewData = spark.sql(\"SELECT * FROM reviewData LIMIT 100000\") \n",
    "print(\"Active Dataset Length: {}\".format(reviewData.count()))\n",
    "reviewData.show()\n",
    "#https://www.amazon.com/gp/vine/help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviewData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Drop uninteresting columns and rows with missing data or error in input. Also filter out rows without verified purchase and drop the verified purchase column. Also convert vine column to integers instead of Y/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|vine|     review_headline|         review_body|review_date|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "|   16414143|R3W4P9UBGNGH1U|B00YL0EKWE|     852431543|LG G4 Case Hard T...|          2|            1|          3|   0|Looks good, funct...|2 issues  -  Once...| 2015-08-31|\n",
      "|   15184378| RY8I449HNXSVF|B00SXRXUKO|     984297154|Tribe AB40 Water ...|          5|            0|          0|   0|          Five Stars|  Fits iPhone 6 well| 2015-08-31|\n",
      "|   10203548|R18TLJYCKJFLSR|B009V5X1CE|     279912704|RAVPower® Element...|          5|            0|          0|   0|       Great charger|Great charger.  I...| 2015-08-31|\n",
      "|     488280|R1NK26SWS53B8Q|B00D93OVF0|     662791300|Fosmon Micro USB ...|          5|            0|          0|   0|          Five Stars|Great for the pri...| 2015-08-31|\n",
      "|   13334021|R11LOHEDYJALTN|B00XVGJMDQ|     421688488|iPhone 6 Case, Vo...|          5|            0|          0|   0|          Five Stars|Great Case, bette...| 2015-08-31|\n",
      "|   27520697|R3ALQVQB2P9LA7|B00KQW1X1C|     554285554|Nokia Lumia 630 R...|          4|            0|          0|   0|          Four Stars|Easy to set up an...| 2015-08-31|\n",
      "|   48086021|R3MWLXLNO21PDQ|B00IP1MQNK|     488006702|Lumsing 10400mah ...|          5|            0|          0|   0|          Five Stars|         Works great| 2015-08-31|\n",
      "|   12738196|R2L15IS24CX0LI|B00HVORET8|     389677711|iPhone 5S Battery...|          5|            0|          0|   0|      So far so good|So far so good. I...| 2015-08-31|\n",
      "|   15867807|R1DJ8976WPWVZU|B00HX3G6J6|     299654876|HTC One M8 Screen...|          3|            0|          0|   0|seems durable but...|seems durable but...| 2015-08-31|\n",
      "|    1972249|R3MRWNNR8CBTB7|B00U4NATNQ|     577878727|S6 Case - Bear Mo...|          5|            0|          0|   0|Super thin, light...|Super thin, light...| 2015-08-31|\n",
      "|   10956619|R1DS6DKTUXAQK3|B00SZEFDH8|     654620704|BLU Studio X, Unl...|          5|            0|          0|   0|          Five Stars|As good as the Sa...| 2015-08-31|\n",
      "|   14805911| RWJM5E0TWUJD2|B00JRJUL9U|     391166958|EZOPower 5-Port U...|          5|            0|          0|   0|            EZOPower|I received this o...| 2015-08-31|\n",
      "|   15611116|R1XTJKDYNCRGAC|B00KQ4T0HE|     481551630|iPhone 6S Case &i...|          1|            0|          0|   0|Very cheap case. ...|Very cheap case. ...| 2015-08-31|\n",
      "|   39298603|R2UZL3DPWEU1XW|B00M0YWKPM|     685107474| iPhone 6s Plus Case|          5|            0|          0|   0|          Five Stars|       son loves it.| 2015-08-31|\n",
      "|   17552454|R2EZXET9KBFFU3|B00KDZEE68|     148320945|zBoost ZB575-A TR...|          1|            0|          0|   0|            One Star|you have to turn ...| 2015-08-31|\n",
      "|   12218556|R26VY1L1FD3LPU|B00BJN45GM|      47788188|OtterBox Defender...|          5|            0|          0|   0|          Five Stars|  Awesome, thank you| 2015-08-31|\n",
      "|   16264332|R1G6333JHJNEUQ|B00Q3I68TU|     974085141|LilGadgets Connec...|          5|            0|          0|   0|Great headphones ...|We love these hea...| 2015-08-31|\n",
      "|    6042304|R2DRG0UZXJQ0PE|B00TN4J1TA|     716174627|Anker Aluminum Mu...|          5|            0|          0|   0|          Five Stars|This is well wort...| 2015-08-31|\n",
      "|    4811414|R3SPEGYBROLSP1|B00QQTFYAA|     194128090|S5 Cable, Galaxy ...|          1|            1|          1|   0|            One Star|This cord didn't ...| 2015-08-31|\n",
      "|   16060664|R3F2UCQP76K15C|B009NLTW60|     135929473|Kinivo BTC450 Blu...|          5|            0|          0|   0|I was later able ...|Initial review af...| 2015-08-31|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the marketplace column and product_category column since they are the same for every product\n",
    "\n",
    "reviewData = reviewData.drop(\"marketplace\",\"product_category\").dropna().dropDuplicates()\n",
    "\n",
    "reviewData = reviewData.filter(reviewData.verified_purchase == \"Y\")\n",
    "reviewData = reviewData.drop(\"verified_purchase\")\n",
    "\n",
    "\n",
    "reviewData_cleared = reviewData.replace([\"Y\",\"N\"],[\"1\",\"0\"],\"vine\")\n",
    "reviewData = reviewData_cleared.withColumn(\"vine\",reviewData_cleared.vine.cast(\"int\"))\n",
    "\n",
    "reviewData = reviewData.withColumn(\"star_rating\", reviewData.star_rating.cast(\"int\"))\n",
    "reviewData = reviewData.withColumn(\"helpful_votes\", reviewData.helpful_votes.cast(\"int\"))\n",
    "reviewData = reviewData.withColumn(\"total_votes\", reviewData.total_votes.cast(\"int\"))\n",
    "\n",
    "reviewData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|count|\n",
      "+----------+-----+\n",
      "|B00QN1T6NM|  287|\n",
      "|B00LBK7OSY|  236|\n",
      "|B00KWR8ME2|  167|\n",
      "|B005X1Y7I2|  158|\n",
      "|B00MQSMEEE|  148|\n",
      "|B00JRGOKQ8|  119|\n",
      "|B00NH13YHK|  118|\n",
      "|B00K4VQZCM|  114|\n",
      "|B00UH8KKA0|  103|\n",
      "|B00UCZGS6S|   97|\n",
      "|B00OT6YUIY|   92|\n",
      "|B00N0YUKEO|   92|\n",
      "|B00OJE1SG8|   86|\n",
      "|B00NH131LY|   83|\n",
      "|B00WUDX250|   83|\n",
      "|B009USAJCC|   78|\n",
      "|B00OQ19QYA|   73|\n",
      "|B00P936188|   73|\n",
      "|B00JM59JPG|   72|\n",
      "|B00PGJWYJ0|   72|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(product_id='B00QN1T6NM', count=287)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = reviewData_cleared.groupBy(\"product_id\").count().sort(\"count\",ascending=False)\n",
    "test.show()\n",
    "test2 = test.filter(test[\"count\"] >= 5)\n",
    "test.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split review column into list and perform various filtering/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|vine|     review_headline|         review_body|review_date|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "|   16414143|R3W4P9UBGNGH1U|B00YL0EKWE|     852431543|LG G4 Case Hard T...|          2|            1|          3|   0|Looks good, funct...|[issues, once, i,...| 2015-08-31|\n",
      "|   15184378| RY8I449HNXSVF|B00SXRXUKO|     984297154|Tribe AB40 Water ...|          5|            0|          0|   0|          Five Stars|[fits, iphone, well]| 2015-08-31|\n",
      "|   10203548|R18TLJYCKJFLSR|B009V5X1CE|     279912704|RAVPower® Element...|          5|            0|          0|   0|       Great charger|[great, charger, ...| 2015-08-31|\n",
      "|     488280|R1NK26SWS53B8Q|B00D93OVF0|     662791300|Fosmon Micro USB ...|          5|            0|          0|   0|          Five Stars|[great, for, the,...| 2015-08-31|\n",
      "|   13334021|R11LOHEDYJALTN|B00XVGJMDQ|     421688488|iPhone 6 Case, Vo...|          5|            0|          0|   0|          Five Stars|[great, case, bet...| 2015-08-31|\n",
      "|   27520697|R3ALQVQB2P9LA7|B00KQW1X1C|     554285554|Nokia Lumia 630 R...|          4|            0|          0|   0|          Four Stars|[easy, to, set, u...| 2015-08-31|\n",
      "|   48086021|R3MWLXLNO21PDQ|B00IP1MQNK|     488006702|Lumsing 10400mah ...|          5|            0|          0|   0|          Five Stars|      [works, great]| 2015-08-31|\n",
      "|   12738196|R2L15IS24CX0LI|B00HVORET8|     389677711|iPhone 5S Battery...|          5|            0|          0|   0|      So far so good|[so, far, so, goo...| 2015-08-31|\n",
      "|   15867807|R1DJ8976WPWVZU|B00HX3G6J6|     299654876|HTC One M8 Screen...|          3|            0|          0|   0|seems durable but...|[seems, durable, ...| 2015-08-31|\n",
      "|    1972249|R3MRWNNR8CBTB7|B00U4NATNQ|     577878727|S6 Case - Bear Mo...|          5|            0|          0|   0|Super thin, light...|[super, thin, lig...| 2015-08-31|\n",
      "|   10956619|R1DS6DKTUXAQK3|B00SZEFDH8|     654620704|BLU Studio X, Unl...|          5|            0|          0|   0|          Five Stars|[as, good, as, th...| 2015-08-31|\n",
      "|   14805911| RWJM5E0TWUJD2|B00JRJUL9U|     391166958|EZOPower 5-Port U...|          5|            0|          0|   0|            EZOPower|[i, received, thi...| 2015-08-31|\n",
      "|   15611116|R1XTJKDYNCRGAC|B00KQ4T0HE|     481551630|iPhone 6S Case &i...|          1|            0|          0|   0|Very cheap case. ...|[very, cheap, cas...| 2015-08-31|\n",
      "|   39298603|R2UZL3DPWEU1XW|B00M0YWKPM|     685107474| iPhone 6s Plus Case|          5|            0|          0|   0|          Five Stars|    [son, loves, it]| 2015-08-31|\n",
      "|   17552454|R2EZXET9KBFFU3|B00KDZEE68|     148320945|zBoost ZB575-A TR...|          1|            0|          0|   0|            One Star|[you, have, to, t...| 2015-08-31|\n",
      "|   12218556|R26VY1L1FD3LPU|B00BJN45GM|      47788188|OtterBox Defender...|          5|            0|          0|   0|          Five Stars|[awesome, thank, ...| 2015-08-31|\n",
      "|   16264332|R1G6333JHJNEUQ|B00Q3I68TU|     974085141|LilGadgets Connec...|          5|            0|          0|   0|Great headphones ...|[we, love, these,...| 2015-08-31|\n",
      "|    6042304|R2DRG0UZXJQ0PE|B00TN4J1TA|     716174627|Anker Aluminum Mu...|          5|            0|          0|   0|          Five Stars|[this, is, well, ...| 2015-08-31|\n",
      "|    4811414|R3SPEGYBROLSP1|B00QQTFYAA|     194128090|S5 Cable, Galaxy ...|          1|            1|          1|   0|            One Star|[this, cord, didn...| 2015-08-31|\n",
      "|   16060664|R3F2UCQP76K15C|B009NLTW60|     135929473|Kinivo BTC450 Blu...|          5|            0|          0|   0|I was later able ...|[initial, review,...| 2015-08-31|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+----+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sqlFunc\n",
    "\n",
    "\n",
    "# make review_body lower case\n",
    "reviewData_temp = reviewData.withColumn(\"review_body\",sqlFunc.lower(sqlFunc.col(\"review_body\")))\n",
    "\n",
    "\n",
    "# Clean data using various regex replacements\n",
    "# 1) Removing non-letters\n",
    "reviewData_temp = reviewData_temp.withColumn(\"review_body\",sqlFunc.regexp_replace(\"review_body\",\"[^a-zA-Z\\\\s]\",\"\"))\n",
    "# 2) Removing repetitive spaces\n",
    "reviewData_temp = reviewData_temp.withColumn(\"review_body\",sqlFunc.regexp_replace(\"review_body\",\"\\s{2,}\",\" \"))\n",
    "# 3) Removing spaces at start of word\n",
    "reviewData_temp = reviewData_temp.withColumn(\"review_body\",sqlFunc.regexp_replace(\"review_body\",\"^(\\s)\",\"\"))\n",
    "# Split review_body on space\n",
    "split_col = sqlFunc.split(reviewData_temp.review_body, ' ')\n",
    "# Replace \" \" with \"\"\n",
    "reviewData_cleaned = reviewData_temp.withColumn(\"review_body\", split_col).replace([\" \"],[\"\"],\"review_body\")\n",
    "reviewData_cleaned.show()\n",
    "#treebankTagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')\n",
    "\n",
    "\n",
    "#def tag_df (s):\n",
    "#    return treebankTagger.tag(s)\n",
    "\n",
    "\n",
    "#tag_udf = udf(tag_df,ArrayType(ArrayType(StringType())))\n",
    "\n",
    "#testCol = testRdd.withColumn(\"review_tagged\",tag_udf(testRdd.review_body))\n",
    "\n",
    "\n",
    "#print(testCol.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|product_id|     review_body_agg|   review_sentiments|\n",
      "+----------+--------------------+--------------------+\n",
      "|0594033926|[[described, deli...|               [0.0]|\n",
      "|0594459451|[[work, nook, col...|               [0.0]|\n",
      "|0594481902|[[bought, item, r...|[-0.7351, 0.0, 0....|\n",
      "|059448426X|[[best, nook, cov...|            [0.1779]|\n",
      "|1059241536|[[poor, broke, day]]|           [-0.7096]|\n",
      "|1059241692|[[hoping, allow, ...|             [0.836]|\n",
      "|1059246147|[[cord, received,...|            [0.3612]|\n",
      "|1059275171|[[worked, several...|           [-0.2263]|\n",
      "|1059356937|            [[love]]|            [0.6369]|\n",
      "|1059359189|[[good, broken, i...|   [-0.0516, 0.6249]|\n",
      "|1059366363|[[good, quality, ...|  [0.8332, 0.0, 0.0]|\n",
      "|1059451522|[[bought, family,...|               [0.0]|\n",
      "|1059641682|[[simple, easy, r...|            [0.4404]|\n",
      "|1059656000|[[exactly, descri...|               [0.0]|\n",
      "|1059742772|[[charger, work, ...|            [0.5106]|\n",
      "|1059844109|[[product, work, ...|           [-0.4215]|\n",
      "|1059844575|[[work, great, ad...|    [0.6249, 0.6249]|\n",
      "|105995656X|[[issue, productb...|            [0.7184]|\n",
      "|1059981424|[[purchased, coll...|            [0.2023]|\n",
      "|1060020823|         [[prefect]]|               [0.0]|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import collect_list,col, udf\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# Initialize Sentiment analyzer \n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "def lemmatizeUDF(revs): \n",
    "    return [[lemmatizer.lemmatize(w) for w in s] for s in revs]\n",
    "\n",
    "def sentimentUDF(revs):\n",
    "    return [sent.polarity_scores(\" \".join(rev))[\"compound\"] for rev in revs]\n",
    "\n",
    "lemmatize_udf = udf(lemmatizeUDF, ArrayType(ArrayType(StringType())))\n",
    "sentiment_udf = udf(sentimentUDF,ArrayType(DoubleType()))\n",
    "\n",
    "# Get product id with reviews\n",
    "idWithReviews = reviewData_cleaned.select(\"product_id\",\"review_body\")\n",
    "\n",
    "# Remove Stop Words from reviews\n",
    "stopWordRemover = StopWordsRemover(inputCol=\"review_body\", outputCol=\"review_noStop\")#,stopWords = stopWords)\n",
    "stopWordsRemoved = stopWordRemover.transform(reviewData_cleaned)\n",
    "\n",
    "# Product ID with the aggregated list of reviews\n",
    "idsWithAllReviews = stopWordsRemoved.groupBy(\"product_id\").agg(collect_list('review_noStop').alias(\"review_body_agg\"))\n",
    "\n",
    "# Perform Lemmatization on all reviews\n",
    "idsWithAllReviews = idsWithAllReviews.withColumn(\"review_body_agg\", lemmatize_udf(idsWithAllReviews.review_body_agg))\n",
    "\n",
    "# Calculate the compound sentiment score for all reviews\n",
    "idsWithAllReviews = idsWithAllReviews.withColumn(\"review_sentiments\", sentiment_udf(idsWithAllReviews.review_body_agg))\n",
    "\n",
    "\n",
    "idsWithAllReviews.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+\n",
      "|product_id|     review_body_agg|         review_flat|              ngrams|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "|0594033926|[[described, deli...|[described, deliv...|[described delive...|\n",
      "|0594459451|[[work, nook, col...|[work, nook, colo...|[work nook, nook ...|\n",
      "|0594481902|[[bought, item, r...|[bought, item, re...|[bought item, ite...|\n",
      "|059448426X|[[best, nook, cov...|[best, nook, cove...|[best nook, nook ...|\n",
      "|1059241536|[[poor, broke, day]]|  [poor, broke, day]|[poor broke, brok...|\n",
      "|1059241692|[[hoping, allow, ...|[hoping, allow, c...|[hoping allow, al...|\n",
      "|1059246147|[[cord, received,...|[cord, received, ...|[cord received, r...|\n",
      "|1059275171|[[worked, several...|[worked, several,...|[worked several, ...|\n",
      "|1059356937|            [[love]]|              [love]|                  []|\n",
      "|1059359189|[[good, broken, i...|[good, broken, it...|[good broken, bro...|\n",
      "|1059366363|[[good, quality, ...|[good, quality, c...|[good quality, qu...|\n",
      "|1059451522|[[bought, family,...|[bought, family, ...|[bought family, f...|\n",
      "|1059641682|[[simple, easy, r...|[simple, easy, re...|[simple easy, eas...|\n",
      "|1059656000|[[exactly, descri...|[exactly, describ...|[exactly describe...|\n",
      "|1059742772|[[charger, work, ...|[charger, work, f...|[charger work, wo...|\n",
      "|1059844109|[[product, work, ...|[product, work, c...|[product work, wo...|\n",
      "|1059844575|[[work, great, ad...|[work, great, adv...|[work great, grea...|\n",
      "|105995656X|[[issue, productb...|[issue, productbu...|[issue productbut...|\n",
      "|1059981424|[[purchased, coll...|[purchased, colle...|[purchased collea...|\n",
      "|1060020823|         [[prefect]]|           [prefect]|                  []|\n",
      "+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "flattened = idsWithAllReviews.withColumn(\"review_flat\", sqlFunc.flatten(\"review_body_agg\"))\n",
    "\n",
    "ngram = NGram(n=2, inputCol= \"review_flat\", outputCol=\"ngrams\")\n",
    "ngramDF = ngram.transform(flattened)\n",
    "\n",
    "ngramDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|ngrams          |count|\n",
      "+----------------+-----+\n",
      "|screen protector|136  |\n",
      "|easy install    |25   |\n",
      "|phone screen    |20   |\n",
      "|dropped phone   |20   |\n",
      "|glass screen    |17   |\n",
      "|easy apply      |15   |\n",
      "|easy put        |15   |\n",
      "|air bubble      |14   |\n",
      "|great product   |12   |\n",
      "|tempered glass  |12   |\n",
      "+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = ngramDF.filter(ngramDF.product_id ==\"B00QN1T6NM\").select(explode(\"ngrams\").alias(\"ngrams\")).groupBy(\"ngrams\").count().orderBy(\"count\",ascending=False)\n",
    "t.show(n=10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|review_flat|count|\n",
      "+-----------+-----+\n",
      "|     screen|  278|\n",
      "|  protector|  195|\n",
      "|      phone|  149|\n",
      "|       easy|   88|\n",
      "|      great|   83|\n",
      "|        one|   70|\n",
      "|      glass|   59|\n",
      "|    product|   57|\n",
      "|     iphone|   46|\n",
      "|     bubble|   41|\n",
      "|        put|   39|\n",
      "|       case|   37|\n",
      "|       good|   36|\n",
      "|    install|   35|\n",
      "|       like|   34|\n",
      "|       time|   31|\n",
      "|       edge|   30|\n",
      "|    dropped|   30|\n",
      "|     really|   27|\n",
      "|      crack|   27|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = ngramDF.filter(ngramDF.product_id ==\"B00QN1T6NM\").select(explode(\"review_flat\").alias(\"review_flat\")).groupBy(\"review_flat\").count().orderBy(\"count\",ascending=False)\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy: 0.7581527233123779\n",
      "product: 0.753288745880127\n",
      "like: 0.7510282397270203\n",
      "protector: 0.7502449154853821\n",
      "easily: 0.7421942353248596\n",
      "wipe: 0.7393692135810852\n",
      "job: 0.7382448315620422\n",
      "came: 0.737947940826416\n",
      "never: 0.7375704646110535\n",
      "still: 0.7339732050895691\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = ngramDF.filter(ngramDF.product_id ==\"B00QN1T6NM\").select(\"review_flat\").rdd.map(lambda x: x.review_flat)\n",
    "\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms(\"look\", 10)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|product_id|             reviews|              ngrams|\n",
      "+----------+--------------------+--------------------+\n",
      "|B00QN1T6NM|       [great, fit!]|        [great fit!]|\n",
      "|B00QN1T6NM|[great, work, per...|[great work, work...|\n",
      "|B00QN1T6NM|              [good]|                  []|\n",
      "|B00QN1T6NM|    [cracked, week.]|     [cracked week.]|\n",
      "|B00QN1T6NM|   [nice, protector]|    [nice protector]|\n",
      "|B00QN1T6NM|[originally, beli...|[originally belie...|\n",
      "|B00QN1T6NM|[like, reviewer, ...|[like reviewer, r...|\n",
      "|B00QN1T6NM|[awesome, product...|[awesome product!...|\n",
      "|B00QN1T6NM|[used, lot, diffe...|[used lot, lot di...|\n",
      "|B00QN1T6NM|[easy, put, came,...|[easy put, put ca...|\n",
      "|B00QN1T6NM|[work, great, rea...|[work great, grea...|\n",
      "|B00QN1T6NM|[got, it-, easy, ...|[got it-, it- eas...|\n",
      "|B00QN1T6NM|[put, correctly,,...|[put correctly,, ...|\n",
      "|B00QN1T6NM|[previous, glass,...|[previous glass, ...|\n",
      "|B00QN1T6NM|[safe, easy, inst...|[safe easy, easy ...|\n",
      "|B00QN1T6NM|[easy!, title, sa...|[easy! title, tit...|\n",
      "|B00QN1T6NM|[dang,, thick, st...|[dang, thick, thi...|\n",
      "|B00QN1T6NM|[love, screen, pr...|[love screen, scr...|\n",
      "|B00QN1T6NM|[clear, instructi...|[clear instructio...|\n",
      "|B00QN1T6NM|        [four, star]|         [four star]|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nt = ngramDF.filter(ngramDF.product_id == \"B00QN1T6NM\" ).select(ngramDF.product_id,explode(ngramDF.ngrams).alias(\"ngrams\"))\\nt = t.select(\"product_id\",array(\"ngrams\").alias(\"ngrams\")).groupBy(\"product_id\").agg(collect_list(\"ngrams\").alias(\"ngrams\"))\\nt.show()'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, array\n",
    "\n",
    "t = flattened.filter(flattened.product_id == \"B00QN1T6NM\").select(flattened.product_id,explode(flattened.review_body_agg).alias(\"reviews\"))\n",
    "#t.show()\n",
    "ngram2 = NGram(n=2, inputCol= \"reviews\", outputCol=\"ngrams\")\n",
    "n = ngram2.transform(t)\n",
    "n.show()\n",
    "\n",
    "\"\"\"\n",
    "t = ngramDF.filter(ngramDF.product_id == \"B00QN1T6NM\" ).select(ngramDF.product_id,explode(ngramDF.ngrams).alias(\"ngrams\"))\n",
    "t = t.select(\"product_id\",array(\"ngrams\").alias(\"ngrams\")).groupBy(\"product_id\").agg(collect_list(\"ngrams\").alias(\"ngrams\"))\n",
    "t.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|product_id|             reviews|              ngrams|\n",
      "+----------+--------------------+--------------------+\n",
      "|B00QN1T6NM|       [great, fit!]|        [great fit!]|\n",
      "|B00QN1T6NM|[great, work, per...|[great work, work...|\n",
      "|B00QN1T6NM|              [good]|                  []|\n",
      "|B00QN1T6NM|    [cracked, week.]|     [cracked week.]|\n",
      "|B00QN1T6NM|   [nice, protector]|    [nice protector]|\n",
      "|B00QN1T6NM|[originally, beli...|[originally belie...|\n",
      "|B00QN1T6NM|[like, reviewer, ...|[like reviewer, r...|\n",
      "|B00QN1T6NM|[awesome, product...|[awesome product!...|\n",
      "|B00QN1T6NM|[used, lot, diffe...|[used lot, lot di...|\n",
      "|B00QN1T6NM|[easy, put, came,...|[easy put, put ca...|\n",
      "|B00QN1T6NM|[work, great, rea...|[work great, grea...|\n",
      "|B00QN1T6NM|[got, it-, easy, ...|[got it-, it- eas...|\n",
      "|B00QN1T6NM|[put, correctly,,...|[put correctly,, ...|\n",
      "|B00QN1T6NM|[previous, glass,...|[previous glass, ...|\n",
      "|B00QN1T6NM|[safe, easy, inst...|[safe easy, easy ...|\n",
      "|B00QN1T6NM|[easy!, title, sa...|[easy! title, tit...|\n",
      "|B00QN1T6NM|[dang,, thick, st...|[dang, thick, thi...|\n",
      "|B00QN1T6NM|[love, screen, pr...|[love screen, scr...|\n",
      "|B00QN1T6NM|[clear, instructi...|[clear instructio...|\n",
      "|B00QN1T6NM|        [four, star]|         [four star]|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import explode\n",
    "wordsData = n#ngramDF.filter(ngramDF.product_id == \"B00QN1T6NM\").select( \"product_id\",explode(ngramDF.review_body).alias(\"review_body\"))\n",
    "wordsData.show()\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\",outputCol=\"tf\")\n",
    "tf = hashingTF.transform(wordsData)\n",
    "#tf.show()\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF(inputCol=\"tf\",outputCol=\"tfidf\",minDocFreq=5).fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "#idfIgnore = IDF(inputCol=\"tf\",outputCol=\"tfidf\",minDocFreq=2).fit(tf)\n",
    "#tfidfIgnore = idfIgnore.transform(tf)\n",
    "#tfidfIgnore.show(n=2,vertical=True,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- reviews: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngrams: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      "\n",
      "-RECORD 0----------------------------------\n",
      " tfidf | (262144,[117225],[0.0])           \n",
      "-RECORD 1----------------------------------\n",
      " tfidf | (262144,[61576,255925],[0.0,0.0]) \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.printSchema()\n",
    "tfidf.select(\"tfidf\").show(2,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|product_id|              ngrams|                 ext|\n",
      "+----------+--------------------+--------------------+\n",
      "|B00QN1T6NM|[this is, is a, a...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[love this, this ...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[it is, is a, a g...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[great product!, ...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[no problem, prob...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[excellent produc...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[such bad, bad qu...|[3.87120101090789...|\n",
      "|B00QN1T6NM|[screen wa, wa a,...|[3.71705033108063...|\n",
      "|B00QN1T6NM|[it a, a very, ve...|[3.71705033108063...|\n",
      "|B00QN1T6NM|[this wa, wa quit...|[3.58351893845611...|\n",
      "|B00QN1T6NM|[to wa, wa easy, ...|[3.58351893845611...|\n",
      "|B00QN1T6NM|[very impressive,...|[3.58351893845611...|\n",
      "|B00QN1T6NM|[it fit, fit my, ...|[3.46573590279972...|\n",
      "|B00QN1T6NM|[shattered when, ...|[3.46573590279972...|\n",
      "|B00QN1T6NM|          [the best]|[3.4657359027997265]|\n",
      "|B00QN1T6NM|[this screen, scr...|[3.3603753871419,...|\n",
      "|B00QN1T6NM|[my first, first ...|[3.26506520733757...|\n",
      "|B00QN1T6NM|[i am, am so, so ...|[3.26506520733757...|\n",
      "|B00QN1T6NM|[i've purchased, ...|[3.26506520733757...|\n",
      "|B00QN1T6NM|[after purchasing...|[3.26506520733757...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def extract_values_from_vec(vector):\n",
    "    return vector.values.tolist()\n",
    "\n",
    "#def extract_values_from_vec_udf(col):\n",
    "#        return udf(extract_values_from_vec,ArrayType(DoubleType()))\n",
    "extract_values_from_vec_udf = udf(extract_values_from_vec,ArrayType(DoubleType()))\n",
    "test = tfidf.withColumn(\"ext\",extract_values_from_vec_udf(\"tfidf\")).select(\"product_id\",\"ngrams\",\"ext\").sort(\"ext\",ascending=False)\n",
    "test.show(vertical=False)\n",
    "#tfidf.select(,\"tfidf\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(product_id='B00QN1T6NM', ngrams=['this is', 'is a', 'a sturdy', 'sturdy protector', 'protector and', 'and the', 'the lifetime', 'lifetime warrenty', 'warrenty is', 'is a', 'a plus.', 'plus. wish', 'wish it', 'it had', 'had just', 'just a', 'a little', 'little more', 'more over', 'over lap', 'lap on', 'on the', 'the screen', 'screen to', 'to make', 'make it', 'it easier', 'easier to', 'to line', 'line everything', 'everything up.', 'up. the', 'the included', 'included cleaner', 'cleaner helped', 'helped get', 'get it', 'it on', 'on perfectly.'], ext=[3.871201010907891, 3.58351893845611, 0.0, 0.0, 0.0, 0.0, 0.0, 3.1780538303479458, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5274662642067964, 0.0, 3.2650652073375754, 2.954910279033736, 0.0, 3.871201010907891, 0.0, 0.0, 3.7170503310806327, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.7170503310806327, 1.7509374747078, 3.3603753871419, 0.0, 0.0, 2.2289732756508, 6.3561076606958915])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-b73fd78cbe71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msynonyms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindSynonyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "inp = ngramDF.filter(ngramDF.product_id ==\"B00YL0EKWE\").select(\"review_flat\").rdd.take(1)[0].review_flat\n",
    "word2vec = Word2Vec()\n",
    "\n",
    "model = word2vec.fit(inp)\n",
    "synonyms = model.findSynonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Input type must be string type but got array<string>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1316.transform.\n: java.lang.IllegalArgumentException: requirement failed: Input type must be string type but got array<string>.\r\n\tat scala.Predef$.require(Predef.scala:281)\r\n\tat org.apache.spark.ml.feature.Tokenizer.validateInputType(Tokenizer.scala:46)\r\n\tat org.apache.spark.ml.UnaryTransformer.transformSchema(Transformer.scala:110)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:75)\r\n\tat org.apache.spark.ml.UnaryTransformer.transform(Transformer.scala:120)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4240e038918c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"review_body\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"words\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwordsData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentenceData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwordsData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.0.0-preview2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Input type must be string type but got array<string>."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = reviewData\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review_body\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "wordsData.show(5)\n",
    "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "#featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "#idfModel = idf.fit(featurizedData)\n",
    "#rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#rescaledData.select(\"review_id\", \"features\").show(n=10,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       star_rating|\n",
      "+-------+------------------+\n",
      "|  count|                36|\n",
      "|   mean| 4.611111111111111|\n",
      "| stddev|1.0495652917219116|\n",
      "|    min|                 1|\n",
      "|    max|                 5|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(review_body=['Great', 'charger.', '', 'I', 'easily', 'get', '3+', 'charges', 'on', 'a', 'Samsung', 'Galaxy', '3.', '', 'Works', 'perfectly', 'for', 'camping', 'trips', 'or', 'long', 'days', 'on', 'the', 'boat.'], star_rating=5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as sqlFunc\n",
    "from pyspark.sql.types import ArrayType\n",
    "testRdd = reviewData_cleared.filter(reviewData_cleared.product_id == \"B009V5X1CE\").select(\"review_body\",\"star_rating\")\n",
    "split_col = sqlFunc.split(testRdd.review_body, ' ')\n",
    "testRdd = testRdd.withColumn(\"review_body\", split_col)\n",
    "testRdd.describe(\"star_rating\").show()\n",
    "#averageRating.take(1)\n",
    "testRdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 issues  -  Once I turned on the circle apps and installed this case,  my battery drained twice as fast as usual.  I ended up turning off the circle apps, which kind of makes the case just a case...  with a hole in it.  Second,  the wireless charging doesn't work.  I have a Motorola 360 watch and a Qi charging pad. The watch charges fine but this case doesn't. But hey, it looks nice.\n",
      "['2', ' ', 'issues', ' ', '', ' ', '-', ' ', '', ' ', 'Once', ' ', 'I', ' ', 'turned', ' ', 'on', ' ', 'the', ' ', 'circle', ' ', 'apps', ' ', 'and', ' ', 'installed', ' ', 'this', ' ', 'case', ',', '', ' ', '', ' ', 'my', ' ', 'battery', ' ', 'drained', ' ', 'twice', ' ', 'as', ' ', 'fast', ' ', 'as', ' ', 'usual', '.', '', ' ', '', ' ', 'I', ' ', 'ended', ' ', 'up', ' ', 'turning', ' ', 'off', ' ', 'the', ' ', 'circle', ' ', 'apps', ',', '', ' ', 'which', ' ', 'kind', ' ', 'of', ' ', 'makes', ' ', 'the', ' ', 'case', ' ', 'just', ' ', 'a', ' ', 'case', '.', '', '.', '', '.', '', ' ', '', ' ', 'with', ' ', 'a', ' ', 'hole', ' ', 'in', ' ', 'it', '.', '', ' ', '', ' ', 'Second', ',', '', ' ', '', ' ', 'the', ' ', 'wireless', ' ', 'charging', ' ', \"doesn't\", ' ', 'work', '.', '', ' ', '', ' ', 'I', ' ', 'have', ' ', 'a', ' ', 'Motorola', ' ', '360', ' ', 'watch', ' ', 'and', ' ', 'a', ' ', 'Qi', ' ', 'charging', ' ', 'pad', '.', '', ' ', 'The', ' ', 'watch', ' ', 'charges', ' ', 'fine', ' ', 'but', ' ', 'this', ' ', 'case', ' ', \"doesn't\", '.', '', ' ', 'But', ' ', 'hey', ',', '', ' ', 'it', ' ', 'looks', ' ', 'nice', '.', '']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "import re\n",
    "#print(mark_negation(testRdd.first().review_body))\n",
    "#k = mark_negation(testRdd.first().review_body)\n",
    "#testRdd= reviewData\n",
    "print(reviewData.first().review_body)\n",
    "print(re.split(\"([,.!? ])\", reviewData.first().review_body))\n",
    "#print([tag_df(row.review_body) for row in testRdd.take(3)])\n",
    "#print(treebankTagger.tag(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 issues  -  once i turned on the circle apps and installed this case,  my battery drained twice as fast as usual', '  i ended up turning off the circle apps, which kind of makes the case just a case', '', '', '  with a hole in it', \"  second,  the wireless charging doesn't work\", '  i have a motorola 360 watch and a qi charging pad', \" the watch charges fine but this case doesn't\", ' but hey, it looks nice', '']\n"
     ]
    }
   ],
   "source": [
    "b = re.split(\"([.!?])\", reviewData.first().review_body)\n",
    "st = sc.parallelize(b).map(lambda x : x.lower()).filter(lambda x: x not in [\".\",\",\",\"!\",\"?\"]) \n",
    "\n",
    "print(st.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Andreas\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['issue', 'turned', 'circle', 'apps', 'installed', 'case', 'battery', 'drained', 'twice', 'fast', 'usual', 'ended', 'turning', 'circle', 'apps', 'kind', 'make', 'case', 'case', 'hole', 'second', 'wireless', 'charging', 'work', 'motorola', 'watch', 'qi', 'charging', 'pad', 'watch', 'charge', 'fine', 'case', 'hey', 'look', 'nice']\n",
      "[('issue', 'NN'), ('turned', 'VBD'), ('circle', 'NN'), ('apps', 'NNS'), ('installed', 'VBD'), ('case', 'NN'), ('battery', 'NN'), ('drained', 'VBD'), ('twice', 'NN'), ('fast', 'NN'), ('ended', 'VBD'), ('turning', 'VBG'), ('circle', 'NN'), ('apps', 'NNS'), ('case', 'NN'), ('case', 'NN'), ('hole', 'NN'), ('second', 'NN'), ('wireless', 'NN'), ('charging', 'VBG'), ('work', 'NN'), ('motorola', 'NN'), ('watch', 'NN'), ('qi', 'NN'), ('charging', 'VBG'), ('pad', 'NN'), ('watch', 'NN'), ('charge', 'NN'), ('fine', 'NN'), ('case', 'NN'), ('hey', 'NN'), ('look', 'NN'), ('nice', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "stemmed = filtered_st.map(lambda w: ps.lemmatize(w))\n",
    "#print(stemmed.collect())\n",
    "tagged = sc.parallelize(tag_df(stemmed.collect())).filter(lambda x: x[1] in [\"NN\",\"VBD\",\"VBG\",\"VV\",\"NNS\"])\n",
    "print(tagged.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "0.4404\n",
      "bad\n",
      "-0.5423\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "st = [\"good\",\"bad\"]\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "for w in st:\n",
    "    print(w)\n",
    "    print(sent.polarity_scores(w)[\"compound\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
